{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Kaviyarasi-Sasiperumal/AI_-Based_-Document-_Search_-and-_Knowledge-_Retrieval_-with-_Conversational_Interface/blob/main/Milestone_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q sentence-transformers faiss-cpu transformers gradio pypdf"
      ],
      "metadata": {
        "id": "f13gKTXPlOOr"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ],
      "metadata": {
        "id": "XA7LqTHEpMeo"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datetime import datetime\n",
        "import numpy as np\n",
        "import faiss\n",
        "import gradio as gr\n",
        "\n",
        "from pypdf import PdfReader\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from transformers import pipeline"
      ],
      "metadata": {
        "id": "ElERSr5epQcI"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embedding_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
        "\n",
        "llm = pipeline(\n",
        "    \"text-generation\",\n",
        "    model=\"google/flan-t5-base\",\n",
        "    max_new_tokens=150\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mawv9Sn9pudQ",
        "outputId": "5e982f19-ae45-49cc-e921-e6e046aa5b71"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cpu\n",
            "The model 'T5ForConditionalGeneration' is not supported for text-generation. Supported models are ['PeftModelForCausalLM', 'ApertusForCausalLM', 'ArceeForCausalLM', 'AriaTextForCausalLM', 'BambaForCausalLM', 'BartForCausalLM', 'BertLMHeadModel', 'BertGenerationDecoder', 'BigBirdForCausalLM', 'BigBirdPegasusForCausalLM', 'BioGptForCausalLM', 'BitNetForCausalLM', 'BlenderbotForCausalLM', 'BlenderbotSmallForCausalLM', 'BloomForCausalLM', 'BltForCausalLM', 'CamembertForCausalLM', 'LlamaForCausalLM', 'CodeGenForCausalLM', 'CohereForCausalLM', 'Cohere2ForCausalLM', 'CpmAntForCausalLM', 'CTRLLMHeadModel', 'Data2VecTextForCausalLM', 'DbrxForCausalLM', 'DeepseekV2ForCausalLM', 'DeepseekV3ForCausalLM', 'DiffLlamaForCausalLM', 'DogeForCausalLM', 'Dots1ForCausalLM', 'ElectraForCausalLM', 'Emu3ForCausalLM', 'ErnieForCausalLM', 'Ernie4_5ForCausalLM', 'Ernie4_5_MoeForCausalLM', 'Exaone4ForCausalLM', 'FalconForCausalLM', 'FalconH1ForCausalLM', 'FalconMambaForCausalLM', 'FlexOlmoForCausalLM', 'FuyuForCausalLM', 'GemmaForCausalLM', 'Gemma2ForCausalLM', 'Gemma3ForConditionalGeneration', 'Gemma3ForCausalLM', 'Gemma3nForConditionalGeneration', 'Gemma3nForCausalLM', 'GitForCausalLM', 'GlmForCausalLM', 'Glm4ForCausalLM', 'Glm4MoeForCausalLM', 'GotOcr2ForConditionalGeneration', 'GPT2LMHeadModel', 'GPT2LMHeadModel', 'GPTBigCodeForCausalLM', 'GPTNeoForCausalLM', 'GPTNeoXForCausalLM', 'GPTNeoXJapaneseForCausalLM', 'GptOssForCausalLM', 'GPTJForCausalLM', 'GraniteForCausalLM', 'GraniteMoeForCausalLM', 'GraniteMoeHybridForCausalLM', 'GraniteMoeSharedForCausalLM', 'HeliumForCausalLM', 'HunYuanDenseV1ForCausalLM', 'HunYuanMoEV1ForCausalLM', 'JambaForCausalLM', 'JetMoeForCausalLM', 'Lfm2ForCausalLM', 'LlamaForCausalLM', 'Llama4ForCausalLM', 'Llama4ForCausalLM', 'LongcatFlashForCausalLM', 'MambaForCausalLM', 'Mamba2ForCausalLM', 'MarianForCausalLM', 'MBartForCausalLM', 'MegaForCausalLM', 'MegatronBertForCausalLM', 'MiniMaxForCausalLM', 'MinistralForCausalLM', 'MistralForCausalLM', 'MixtralForCausalLM', 'MllamaForCausalLM', 'ModernBertDecoderForCausalLM', 'MoshiForCausalLM', 'MptForCausalLM', 'MusicgenForCausalLM', 'MusicgenMelodyForCausalLM', 'MvpForCausalLM', 'NemotronForCausalLM', 'OlmoForCausalLM', 'Olmo2ForCausalLM', 'Olmo3ForCausalLM', 'OlmoeForCausalLM', 'OpenLlamaForCausalLM', 'OpenAIGPTLMHeadModel', 'OPTForCausalLM', 'PegasusForCausalLM', 'PersimmonForCausalLM', 'PhiForCausalLM', 'Phi3ForCausalLM', 'Phi4MultimodalForCausalLM', 'PhimoeForCausalLM', 'PLBartForCausalLM', 'ProphetNetForCausalLM', 'QDQBertLMHeadModel', 'Qwen2ForCausalLM', 'Qwen2MoeForCausalLM', 'Qwen3ForCausalLM', 'Qwen3MoeForCausalLM', 'Qwen3NextForCausalLM', 'RecurrentGemmaForCausalLM', 'ReformerModelWithLMHead', 'RemBertForCausalLM', 'RobertaForCausalLM', 'RobertaPreLayerNormForCausalLM', 'RoCBertForCausalLM', 'RoFormerForCausalLM', 'RwkvForCausalLM', 'SeedOssForCausalLM', 'SmolLM3ForCausalLM', 'Speech2Text2ForCausalLM', 'StableLmForCausalLM', 'Starcoder2ForCausalLM', 'TransfoXLLMHeadModel', 'TrOCRForCausalLM', 'VaultGemmaForCausalLM', 'WhisperForCausalLM', 'XGLMForCausalLM', 'XLMWithLMHeadModel', 'XLMProphetNetForCausalLM', 'XLMRobertaForCausalLM', 'XLMRobertaXLForCausalLM', 'XLNetLMHeadModel', 'xLSTMForCausalLM', 'XmodForCausalLM', 'ZambaForCausalLM', 'Zamba2ForCausalLM'].\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "system_logs = []\n",
        "uploaded_docs = []\n",
        "\n",
        "documents = []\n",
        "embeddings = []\n",
        "\n",
        "DIM = 384\n",
        "index = faiss.IndexFlatL2(DIM)"
      ],
      "metadata": {
        "id": "eGNP3kBAp8HR"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def log(msg):\n",
        "    t = datetime.now().strftime(\"%H:%M:%S\")\n",
        "    system_logs.append(f\"[{t}] {msg}\")\n",
        "    return \"\\n\".join(system_logs)"
      ],
      "metadata": {
        "id": "zuvhXl9YqAUj"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_text(pdf_path):\n",
        "    reader = PdfReader(pdf_path)\n",
        "    text = \"\"\n",
        "    for page in reader.pages:\n",
        "        if page.extract_text():\n",
        "            text += page.extract_text() + \"\\n\"\n",
        "    return text"
      ],
      "metadata": {
        "id": "WIaqnXgLqE-f"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def chunk_text(text, size=1000):\n",
        "    return [text[i:i+size] for i in range(0, len(text), size)]"
      ],
      "metadata": {
        "id": "CGiRC1TGqKqe"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def ingest_document(file):\n",
        "    uploaded_docs.append(file.name)\n",
        "    log(f\"Ingesting {file.name}\")\n",
        "\n",
        "    text = extract_text(file.name)\n",
        "    log(\"Text extracted\")\n",
        "\n",
        "    chunks = chunk_text(text)\n",
        "    log(f\"{len(chunks)} chunks created\")\n",
        "\n",
        "    vecs = embedding_model.encode(chunks)\n",
        "    log(\"Embeddings generated\")\n",
        "\n",
        "    for c, v in zip(chunks, vecs):\n",
        "        documents.append(c)\n",
        "        embeddings.append(v)\n",
        "\n",
        "    index.add(np.array(embeddings).astype(\"float32\"))\n",
        "    log(\"Stored in vector index\")\n",
        "\n",
        "    return \"\\n\".join(system_logs), \"\\n\".join(uploaded_docs)"
      ],
      "metadata": {
        "id": "1lzQtyJeqPXV"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def retrieve(query, k=6):\n",
        "    q_vec = embedding_model.encode([query]).astype(\"float32\")\n",
        "    _, ids = index.search(q_vec, k)\n",
        "    return [documents[i] for i in ids[0] if i < len(documents)]"
      ],
      "metadata": {
        "id": "epG8aUo8qTjf"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def chatbot(query):\n",
        "    chunks = retrieve(query, k=6)\n",
        "\n",
        "    if not chunks:\n",
        "        return \"No answer found in documents.\", \"\"\n",
        "\n",
        "    context = \"\\n\".join(chunks)\n",
        "\n",
        "    prompt = f\"\"\"\n",
        "You are a study assistant.\n",
        "Read the entire context carefully and explain the concept completely.\n",
        "\n",
        "Context:\n",
        "{context}\n",
        "\n",
        "Question:\n",
        "{query}\n",
        "\n",
        "Give a clear, complete, and meaningful explanation:\n",
        "\"\"\"\n",
        "    result = llm(prompt)[0][\"generated_text\"]\n",
        "\n",
        "    return result.strip(), \"\\n\\n\".join(chunks)"
      ],
      "metadata": {
        "id": "P_dYgdAiqXRT"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with gr.Blocks(theme=gr.themes.Soft()) as app:\n",
        "    gr.Markdown(\"# ðŸ“„ Document Chatbot â€“ Milestone 1\")\n",
        "    gr.Markdown(\"Document ingestion, indexing, and basic chatbot\")\n",
        "\n",
        "    with gr.Row():\n",
        "        with gr.Column():\n",
        "            file_input = gr.File(label=\"Upload PDF\")\n",
        "            upload_btn = gr.Button(\"Upload & Index\")\n",
        "\n",
        "        with gr.Column():\n",
        "            status_box = gr.Textbox(\n",
        "                label=\"System Status\",\n",
        "                lines=10,\n",
        "                interactive=False\n",
        "            )\n",
        "\n",
        "    doc_list = gr.Textbox(\n",
        "        label=\"Uploaded Documents\",\n",
        "        lines=4,\n",
        "        interactive=False\n",
        "    )\n",
        "\n",
        "    upload_btn.click(\n",
        "        ingest_document,\n",
        "        inputs=file_input,\n",
        "        outputs=[status_box, doc_list]\n",
        "    )\n",
        "\n",
        "    gr.Markdown(\"---\")\n",
        "\n",
        "    question = gr.Textbox(label=\"Ask a Question\")\n",
        "    sources = gr.Textbox(label=\"Retrieved Answer\", lines=4)\n",
        "\n",
        "    ask_btn = gr.Button(\"Ask\")\n",
        "    ask_btn.click(chatbot, inputs=question, outputs=[answer, sources])\n",
        "\n",
        "app.launch(share=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 611
        },
        "id": "pa-R6DHQqbP-",
        "outputId": "940f6044-ece5-48c1-cf4c-4ea832e59ad8"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://ab1c8ddbd2a6b9b670.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://ab1c8ddbd2a6b9b670.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "mount_file_id": "1TIZtb0lQ3I6MWTGAH27mR2bkuoDW6jvQ",
      "authorship_tag": "ABX9TyMgYnP99pM6ZHO/SDIs9K7N",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}